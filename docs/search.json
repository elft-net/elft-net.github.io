[
  {
    "objectID": "journal.html",
    "href": "journal.html",
    "title": "The Devops Society",
    "section": "",
    "text": "The Q-Factor\n\n\n\n\n\n\namazon\n\n\naws\n\n\nai\n\n\n\n\n\n\n\n\n\nSep 25, 2024\n\n\nPeter Mostert\n\n\n\n\n\n\n\n\n\n\n\n\nRe-installing Nix on macOS\n\n\n\n\n\n\nnix\n\n\nnix-darwin\n\n\ndevbox\n\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\nPeter Mostert\n\n\n\n\n\n\n\n\n\n\n\n\nSome Thoughts about AI.\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 23, 2024\n\n\nPeter Mostert\n\n\n\n\n\n\n\n\n\n\n\n\nPost With Code\n\n\n\n\n\n\ncode\n\n\nanalysis\n\n\nr\n\n\n\n\n\n\n\n\n\nSep 20, 2024\n\n\nPeter Mostert\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\nSep 17, 2024\n\n\nPeter Mostert\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About\nIn an effort to get aquainted with Quarto and its Features this site has been published to Document the journey. This caused the origin of this site and if things go according to plan then the Journal will get decorated with Devops rekated articles.\nAt some point in time regular life will probably get in the way blocking our endeavours to explore new technologies and workflows. But then, according to Albert, time slows down when moving fast so let’s do just that and quickly push things forward."
  },
  {
    "objectID": "posts/ai/ai-intro.html",
    "href": "posts/ai/ai-intro.html",
    "title": "Some Thoughts about AI.",
    "section": "",
    "text": "Within the realm of Artificial Intelligence (AI), the journey from basic learning patterns to cutting-edge generative models has been nothing short of transformative. Understanding the evolution of AI while keeping the context of generative AI in mind, sheds light on the capabilities and possibilities of this current technology hype.\nBasic Learning Patterns and Inference\nAt the core of AI’s capabilities lie the basic learning patterns and inference stages. Initially, AI algorithms were designed to analyze data, recognize patterns, and make predictions based on these patterns. This process involved processing vast amounts of data to identify correlations and extract valuable insights.\nWith the foundational knowledge gained from learning patterns, AI models enter the inference stage. Here, they leverage their understanding of the patterns to make informed decisions or evaluations based on new data. This capability enable AI systems to apply their learned knowledge in real-world scenarios.\nGenerative AI\nIn recent years, generative AI has emerged as the new frontier in artificial intelligence. Unlike traditional AI models that focus on analyzing existing data, generative AI is capable of creating new data and content. These models have the ability to generate text, images, music, and even videos that mimic human creativity and imagination.\nOne of the key aspects of generative AI is the incorporation of a common knowledge base (LLM) built during the learning stage. By providing context to the model in the inferencing stage, AI systems develop some form of awareness that enables them to generate outcomes. This contextual understanding allows generative AI models to produce content that aligns with human expectations.\nStatistical Approach and Hallucinated Outcomes\nGenerative AI operates on a statistical approach that enables it to generate diverse and sometimes unexpected outcomes. The statitical hustling of learned content introduces the sugestion of being creative but also introduces the concept of hallucinated outcomes, where AI models produce results that may not have a direct correlation with the input data. These hallucinated outcomes can manifest in various forms, such as creative variations of existing content or entirely novel creations that push the boundaries of human imagination.\nWhile the potential of generative AI is undeniably captivating, challenges lie ahead that demand careful consideration. One such challenge arises from the fact that many Large Language Models (LLMs) are trained on a broad, generic knowledge base. As a result, these models may lack detailed information about recent developments or company-specific nuances. This limitation raises a critical question: Can businesses rely on the outcomes of generative AI for strategic decision making? Understanding the potential risks associated with hallucinated outcomes due to the lack of specific knowledge input is essential to leveraging AI effectively.\nImagine a scenario where a company turns to generative AI to generate market analysis reports or make predictions about future trends. In the absence of real-time, updated data tailored to the company’s needs, the chances of encountering hallucinated outcomes or inaccurate conclusions become more pronounced. The AI model, disconnected from the latest industry insights or internal company dynamics, may inadvertently lead decision-makers astray by producing results that are not aligned with the current reality.\nSo, how can organizations mitigate these risks and enhance the accuracy of generative AI models in practical applications? This question beckons a crucial discussion on the need for fine-tuning AI systems, integrating domain-specific knowledge, and implementing robust validation processes to ensure that the outcomes derived from generative AI align with reality. In the next phase of AI advancement, addressing these challenges and fortifying generative models with targeted information could pave the way for more reliable, context-aware AI solutions that empower organizations to make informed decisions with confidence."
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code.\nx &lt;- 1\ny &lt;- 2\nx + y"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Peter Mostert",
    "section": "",
    "text": "Digital Engineer @ TechNative | CI/CD, ICT-Strategy, MicroServices"
  },
  {
    "objectID": "posts/nix/nix-reinstall.html",
    "href": "posts/nix/nix-reinstall.html",
    "title": "Re-installing Nix on macOS",
    "section": "",
    "text": "Needed to re-install Nix after having deployed an update that didn’t work out. Just could not spend the time to figure out why access to the nix deamon was denied in the new version. Re-installing provided me with an opportunity to get my Nix-installation in compliance with the rest of the organization.\nMy journey started here: https://zero-to-nix.com/concepts/nix-installer. The script failed while the /nix file system could not be replaced and there were some users and a group that stood in the way. Got a clean setup by doing:\ndiskutil apfs deleteVolume \"Nix Store\"\nsudo dscl . -delete /Groups/nixbld\nfor i in $(seq 12 32); do sudo dscl . -delete /Users/_nixbld$i; done\nAfter that the installation worked (I had to try twice for some reason).\n[mos@chap ~/Repo/play/devops-directive-kubernetes-course]$ curl --proto '=https' --tlsv1.2 -sSf -L https://install.determinate.systems/nix | sh -s -- install\ninfo: downloading installer (https://install.determinate.systems/nix/tag/v0.26.2/nix-installer-x86_64-darwin)\n INFO nix-installer v0.26.2\n`nix-installer` needs to run as `root`, attempting to escalate now via `sudo`...\nPassword:\n INFO nix-installer v0.26.2\nNix install plan (v0.26.2)\nPlanner: macos (with default settings)\n\nPlanned actions:\n* Create an encrypted APFS volume `Nix Store` for Nix on `disk1` and add it to `/etc/fstab` mounting on `/nix`\n* Extract the bundled Nix (originally from /nix/store/zw0glikil8ncwb6r7bgdrilk50cval8i-nix-binary-tarball-2.24.7/nix-2.24.7-x86_64-darwin.tar.xz)\n* Create a directory tree in `/nix`\n* Move the downloaded Nix into `/nix`\n* Create build users (UID 351-382) and group (GID 350)\n* Configure Time Machine exclusions\n* Setup the default Nix profile\n* Place the Nix configuration in `/etc/nix/nix.conf`\n* Configure the shell profiles\n* Configuring zsh to support using Nix in non-interactive shells\n* Create a `launchctl` plist to put Nix into your PATH\n* Configure upstream Nix daemon service\n* Remove directory `/nix/temp-install-dir`\n\n\nProceed? ([Y]es/[n]o/[e]xplain): Y\n INFO Step: Create an encrypted APFS volume `Nix Store` for Nix on `disk1` and add it to `/etc/fstab` mounting on `/nix`\n INFO Step: Provision Nix\n INFO Step: Create build users (UID 351-382) and group (GID 350)\n INFO Step: Configure Time Machine exclusions\n INFO Step: Configure Nix\n INFO Step: Configuring zsh to support using Nix in non-interactive shells\n INFO Step: Create a `launchctl` plist to put Nix into your PATH\n INFO Step: Configure upstream Nix daemon service\n INFO Step: Remove directory `/nix/temp-install-dir`\nNix was installed successfully!\nTo get started using Nix, open a new shell or run `. /nix/var/nix/profiles/default/etc/profile.d/nix-daemon.sh`\n\n[mos@chap ~/Repo/play/devops-directive-kubernetes-course]$\nnix-installer-x86_64-darwin\n[mos@chap ~/.config]$ mkdir -p ~/.config/nix-darwin                                             (:|✔)  4:00PM\n[mos@chap ~/.config]$ cd nix-darwin/                                                            (:|✔)  4:01PM\n[mos@chap ~/.config/nix-darwin]$ nix flake init -t nix-darwin                                   (:|✔)  4:01PM\nwrote: /Users/mos/.config/nix-darwin/flake.nix\n[mos@chap ~/.config/nix-darwin]$ sed -i '' \"s/simple/$(scutil --get LocalHostName)/\" flake.nix  (:|✔)  4:02PM\n[mos@chap ~/.config/nix-darwin]$ nix run nix-darwin -- switch --flake ~/.config/nix-darwin#chap\nmoving forward\nnix-darwin and nix can be setup to control a ton of settings. For now thing like home-manager (controling user specific configuration settings) and brew repositories (enabling to install the brew applications not available in the nix-store) were not merged into the config just to not get overwhealmed.\nHowever devbox now runs upon the nix installation described above. DevBox is a tool that can configures your shell on a project bases. If the project directory contains a devbox.json file then devbox handles this in a flake like approach. devbox shell installs the packages mentioned in devbox.json into your current shell. This is my main use-case atm.\nsample devbox.json\n{\n  \"$schema\": \"https://raw.githubusercontent.com/jetify-com/devbox/0.10.7/.schema/devbox.schema.json\",\n  \"packages\": [\n    \"civo@latest\",\n    \"envsubst@latest\",\n    \"gh@latest\",\n    \"go@latest\",\n    \"go-task@latest\",\n    \"path:gcloud#google-cloud-sdk\",\n    \"gum@latest\",\n    \"jq@latest\",\n    \"k9s@latest\",\n    \"kind@latest\",\n    \"kluctl@latest\",\n    \"ko@latest\",\n    \"kubectl@latest\",\n    \"kubectx@latest\",\n    \"kubernetes-helm@latest\",\n    \"kustomize@latest\",\n    \"oras@latest\",\n    \"nodejs_20@latest\",\n    \"poetry@latest\",\n    \"python312@latest\",\n    \"tilt@latest\",\n    \"yq-go@latest\",\n    \"act@latest\",\n    \"kubent@latest\"\n  ],\n  \"shell\": {\n    \"init_hook\": [\n      \"export GOBIN=$(git rev-parse --show-toplevel)/bin\",\n      \"export PATH=$GOBIN:$PATH\",\n      \"go install sigs.k8s.io/cloud-provider-kind@v0.2.0\"\n    ],\n    \"scripts\": {\n      \"test\": [\"echo \\\"Error: no test specified\\\" && exit 1\"]\n    }\n  }\n}"
  },
  {
    "objectID": "posts/ai/amazon-q.html",
    "href": "posts/ai/amazon-q.html",
    "title": "The Q-Factor",
    "section": "",
    "text": "While our development pallet grows more generative and creative the providers got stuck into legacy branding patterns. The twitter guy is into X, microSoft brands all the things Copilot and Amazon is into Q.\nOff all AI-providers Amazon Q provided the highest barrier of entry by far and still lacks support for the tools I generally use. But putting my bias aside I should be able to get something going so let’s see whats the Amazon-Q interface has to offer.\nDon’t let the quantity of this post scare you away. Most of the content got generated by Q, has been added while it has relevancy but can be easily skimmed over!\nAmazon Q for command line\nThe Amazon docs portal provides a document describing how to get things going if using macOS.\nBesides a mac this requires a subscription for Builder ID which seems to be free. After install a q command is added to the command pallet. At first sight the most appealing options are the CLI-completions and the CLI-chatbot. Lets play a bit with the latter.\nThe Q-bot\nLets first figure out if Q “knows” about Terraform which is a Third Party Tool (their native IaC tool is called CloudFormation) and Talos (available on AWS as a community AMI).\n[mos@chap ~]$ q chat\nHi, I’m Amazon Q. I can answer questions about your shell and CLI tools! You can include additional context by adding the following to your prompt:\n@history to pass your shell history @git to pass information about your current git repository @env to pass your shell environment\n\ncan you give me some advice on terraform code in AWS\n\nCertainly! I’d be happy to provide some advice on using Terraform with AWS. Here are some best practices and tips to help you write effective and maintainable Terraform code for AWS: [1]\n\nUse modules: Organize your code into reusable modules to improve maintainability and promote code reuse across different projects or environments.\nFollow a consistent naming convention: Use a consistent naming scheme for your resources, variables, and outputs to improve readability and maintainability.\nUse remote state storage: Store your Terraform state files in a remote backend like S3 with DynamoDB for state locking. This enables collaboration and helps prevent conflicts. [2]\nImplement proper state management: Use workspaces or separate state files for different environments (dev, staging, prod) to keep your infrastructure isolated.\nUse variables and locals: Leverage variables for values that might change and locals for computed values or to improve readability.\nImplement proper version control: Use version control systems like Git to manage your Terraform code and track changes over time. [3]\nUse data sources: Utilize data sources to fetch existing AWS resources instead of hardcoding values.\nImplement proper tagging: Use tags for your AWS resources to improve organization, cost allocation, and resource management.\nUse terraform-docs: Generate documentation automatically for your Terraform modules using tools like terraform-docs.\nImplement proper error handling: Use the count parameter or for_each to handle conditional resource creation and avoid errors.\nUse the AWS provider alias: When working with multiple AWS accounts or regions, use provider aliases to manage resources across different environments.\nImplement proper security practices: Use IAM roles and policies to manage access, and avoid hardcoding sensitive information in your Terraform code.\n\nHere’s a simple example that demonstrates some of these practices:\n# Define the AWS provider\nprovider \"aws\" {\n  region = var.aws_region\n}\n\n# Use a module for creating an EC2 instance\nmodule \"ec2_instance\" {\n  source  = \"terraform-aws-modules/ec2-instance/aws\"\n  version = \"~&gt; 3.0\"\n\n  name = \"my-ec2-instance\"\n\n  ami                    = data.aws_ami.amazon_linux_2.id\n  instance_type          = var.instance_type\n  key_name               = var.key_name\n  monitoring             = true\n  vpc_security_group_ids = [aws_security_group.allow_ssh.id]\n  subnet_id              = var.subnet_id\n\n  tags = {\n    Terraform   = \"true\"\n    Environment = var.environment\n  }\n}\n\n# Use a data source to fetch the latest Amazon Linux 2 AMI\ndata \"aws_ami\" \"amazon_linux_2\" {\n  most_recent = true\n  owners      = [\"amazon\"]\n\n  filter {\n    name   = \"name\"\n    values = [\"amzn2-ami-hvm-*-x86_64-gp2\"]\n  }\n}\n\n# Create a security group\nresource \"aws_security_group\" \"allow_ssh\" {\n  name        = \"allow_ssh\"\n  description = \"Allow SSH inbound traffic\"\n  vpc_id      = var.vpc_id\n\n  ingress {\n    description = \"SSH from VPC\"\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [var.vpc_cidr]\n  }\n\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  tags = {\n    Name = \"allow_ssh\"\n  }\n}\n\n# Define variables\nvariable \"aws_region\" {\n  description = \"AWS region\"\n  type        = string\n  default     = \"us-west-2\"\n}\n\nvariable \"instance_type\" {\n  description = \"EC2 instance type\"\n  type        = string\n  default     = \"t2.micro\"\n}\n\nvariable \"key_name\" {\n  description = \"Key pair name\"\n  type        = string\n}\n\nvariable \"subnet_id\" {\n  description = \"Subnet ID\"\n  type        = string\n}\n\nvariable \"vpc_id\" {\n  description = \"VPC ID\"\n  type        = string\n}\n\nvariable \"vpc_cidr\" {\n  description = \"VPC CIDR block\"\n  type        = string\n}\n\nvariable \"environment\" {\n  description = \"Environment name\"\n  type        = string\n  default     = \"dev\"\n}\n\n# Output the instance ID\noutput \"instance_id\" {\n  description = \"ID of the EC2 instance\"\n  value       = module.ec2_instance.id\n}\nThis example demonstrates the use of modules, data sources, variables, and proper resource organization. Remember to adapt this code to your specific needs and always follow AWS best practices for security and resource management.\n1 https://aws.amazon.com/about-aws/whats-new/2023/11/amazon-codecatalyst-supports-terraform/ 2 https://repost.aws/articles/AR1Iq4DrJtThK4K2svXyuF3w/using-codecatalyst-to-deploy-terraform-infrastructure-as-code 3 https://docs.aws.amazon.com/prescriptive-guidance/latest/getting-started-terraform/introduction.html\nOoempf, that does not disappoint. Now let’s try somethinig more specific.\nIaC for an Instance running Talos\n\nI would like to have the terraform code that provides me with a talos image running the control plane as well as the worker plane for a medium developer setup. The terraform code should provide an EC2 resource with sufficient capacity to run a medium developer instance. Initially it can run in the default vpc. Can you provide the terraform code needed to provision this ?\n\nCertainly! I can provide you with Terraform code to set up an EC2 instance running a Talos image for both the control plane and worker plane in a medium developer setup. We’ll use the default VPC for simplicity. Here’s the Terraform code to achieve this:\nprovider \"aws\" {\n  region = \"eu-central-1\"  # Change this to your preferred region\n}\n\ndata \"aws_vpc\" \"default\" {\n  default = true\n}\n\ndata \"aws_subnets\" \"default\" {\n  filter {\n    name   = \"vpc-id\"\n    values = [data.aws_vpc.default.id]\n  }\n}\n\nresource \"aws_security_group\" \"talos_sg\" {\n  name        = \"talos-security-group\"\n  description = \"Security group for Talos Kubernetes cluster\"\n  vpc_id      = data.aws_vpc.default.id\n\n  # Allow all outbound traffic\n  egress {\n    from_port   = 0\n    to_port     = 0\n    protocol    = \"-1\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  # Allow inbound SSH access\n  ingress {\n    from_port   = 22\n    to_port     = 22\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  # Allow Kubernetes API server\n  ingress {\n    from_port   = 6443\n    to_port     = 6443\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n\n  # Allow Talos API\n  ingress {\n    from_port   = 50000\n    to_port     = 50001\n    protocol    = \"tcp\"\n    cidr_blocks = [\"0.0.0.0/0\"]\n  }\n}\n\ndata \"aws_ami\" \"talos\" {\n  most_recent = true\n  owners      = [\"540036508848\"] # Talos Systems' AWS account ID\n\n  filter {\n    name   = \"name\"\n    values = [\"talos-amd64-*\"]\n  }\n\n  filter {\n    name   = \"virtualization-type\"\n    values = [\"hvm\"]\n  }\n}\n\nresource \"aws_instance\" \"talos_node\" {\n  ami           = data.aws_ami.talos.id\n  instance_type = \"t3.medium\"  # Adjust as needed for your \"medium developer setup\"\n\n  vpc_security_group_ids = [aws_security_group.talos_sg.id]\n  subnet_id              = data.aws_subnets.default.ids[0]\n\n  root_block_device {\n    volume_type = \"gp3\"\n    volume_size = 50  # Adjust as needed\n  }\n\n  tags = {\n    Name = \"talos-node\"\n  }\n\n  user_data = &lt;&lt;-EOF\n              {\n                \"version\": \"v1alpha1\",\n                \"persist\": true,\n                \"machine\": {\n                  \"type\": \"controlplane\",\n                  \"token\": \"${random_password.talos_token.result}\",\n                  \"ca\": {\n                    \"crt\": \"${base64encode(tls_self_signed_cert.talos_ca.cert_pem)}\",\n                    \"key\": \"${base64encode(tls_private_key.talos_ca.private_key_pem)}\"\n                  }\n                },\n                \"cluster\": {\n                  \"id\": \"${random_uuid.cluster_id.result}\",\n                  \"secret\": \"${random_password.cluster_secret.result}\",\n                  \"controlPlane\": {\n                    \"endpoint\": \"https://${aws_instance.talos_node.public_ip}:6443\"\n                  }\n                }\n              }\n              EOF\n}\n\nresource \"random_password\" \"talos_token\" {\n  length  = 32\n  special = false\n}\n\nresource \"random_uuid\" \"cluster_id\" {}\n\nresource \"random_password\" \"cluster_secret\" {\n  length  = 32\n  special = false\n}\n\nresource \"tls_private_key\" \"talos_ca\" {\n  algorithm = \"RSA\"\n  rsa_bits  = 4096\n}\n\nresource \"tls_self_signed_cert\" \"talos_ca\" {\n  private_key_pem = tls_private_key.talos_ca.private_key_pem\n\n  subject {\n    common_name = \"talos\"\n  }\n\n  validity_period_hours = 8760  # 1 year\n\n  allowed_uses = [\n    \"key_encipherment\",\n    \"digital_signature\",\n    \"server_auth\",\n    \"client_auth\",\n  ]\n\n  is_ca_certificate = true\n}\n\noutput \"talos_node_public_ip\" {\n  value = aws_instance.talos_node.public_ip\n}\n\noutput \"talos_node_private_ip\" {\n  value = aws_instance.talos_node.private_ip\n}\n\nThis Terraform code does the following:\n\nSets up the AWS provider (you’ll need to configure your AWS credentials).\nUses the default VPC and subnet.\nCreates a security group allowing necessary inbound traffic for Talos and Kubernetes.\nFinds the latest Talos AMI.\nCreates an EC2 instance using the Talos AMI with a t3.medium instance type (adjust as needed).\nGenerates necessary tokens, IDs, and certificates for Talos configuration.\nProvides the Talos configuration as user data to the EC2 instance.\n\nTo use this code:\n\nSave it to a file with a .tf extension (e.g., talos_setup.tf).\nRun terraform init to initialize the Terraform working directory. [1]\nRun terraform plan to see the execution plan.\nRun terraform apply to create the resources.\n\nAfter applying, you’ll get the public and private IP addresses of the Talos node as outputs.\nNote that this setup creates a single node acting as both control plane and worker. For a production environment, you’d want to separate these and create multiple nodes. Also, remember to secure your cluster properly, as this example opens some ports to 0.0.0.0/0 for simplicity.\nTo fully set up and interact with your Talos cluster, you’ll need to install the Talos CLI (talosctl) on your local machine and perform additional configuration steps. Refer to the Talos documentation for post- installation steps and how to interact with your new cluster.\n1 https://repost.aws/articles/ARcZH31DCIS-K3Ivns0ey-dA/automating-ec2-instance-naming-with-terraform-and-user-data-a-solution-for-dynamic-environments\n\n\n\nGood Practices\nNone of the good practices of the generic example got implemented in the specific code. Moreover port 22 is opened for the public while Talos does not require this afaik.\nHad to do some fiddling for to get the correct ami selected and removed a self reference to the end-point but then the plan seemd to come together.\nDo you want to perform these actions?\n  Terraform will perform the actions described above.\n  Only 'yes' will be accepted to approve.\n\n  Enter a value: yes\n\nrandom_uuid.cluster_id: Creating...\ntls_private_key.talos_ca: Creating...\nrandom_uuid.cluster_id: Creation complete after 0s [id=6352c2f2-f216-1659-eb51-d019287cb7bc]\nrandom_password.talos_token: Creating...\nrandom_password.cluster_secret: Creating...\nrandom_password.talos_token: Creation complete after 0s [id=none]\nrandom_password.cluster_secret: Creation complete after 0s [id=none]\naws_security_group.talos_sg: Creating...\ntls_private_key.talos_ca: Creation complete after 2s [id=e6c78d34a473c3e3498202797146adc7e644d339]\ntls_self_signed_cert.talos_ca: Creating...\ntls_self_signed_cert.talos_ca: Creation complete after 0s [id=160873380053405108687366549035503198069]\naws_security_group.talos_sg: Creation complete after 2s [id=sg-041c4f1a231aaa943]\naws_instance.talos_node: Creating...\naws_instance.talos_node: Still creating... [10s elapsed]\naws_instance.talos_node: Creation complete after 13s [id=i-00c4cb23e04838b0f]\n\nApply complete! Resources: 7 added, 0 changed, 0 destroyed.\n\nOutputs:\n\ntalos_node_private_ip = \"172.31.7.6\"\ntalos_node_public_ip = \"3.69.46.111\"\n[mos@chap ~/…/play/devops-directive-terraform-course/10-aws-talos]$\nIn theory this should enable us to setup the Talos cluster but trying to do so returns:\nUnable to connect\n\nAn error occurred during a connection to ec2-3-69-46-111.eu-central-1.compute.amazonaws.com:6443.\n\n    The site could be temporarily unavailable or too busy. Try again in a few moments.\n    If you are unable to load any pages, check your computer’s network connection.\n    If your computer or network is protected by a firewall or proxy, make sure that Firefox is permitted to access the web.\nNot a good sign . . . trying to bootstrap the Talos node anyway caused some further havoc:\n[mos@chap ~/…/play/devops-directive-terraform-course/10-aws-talos]$ talosctl gen config talos-cluster https://3.69.46.111:6443\ngenerating PKI and tokens\nCreated /Users/mos/Repo/play/devops-directive-terraform-course/10-aws-talos/controlplane.yaml\nCreated /Users/mos/Repo/play/devops-directive-terraform-course/10-aws-talos/worker.yaml\nCreated /Users/mos/Repo/play/devops-directive-terraform-course/10-aws-talos/talosconfig\n[mos@chap ~/…/play/devops-directive-terraform-course/10-aws-talos]$ talosctl apply-config --insecure -n 3.69.46.111 --file controlplane.yaml                                                                                      \nerror applying new configuration: rpc error: code = Unavailable desc = connection error: desc = \"transport: Error while dialing: dial tcp 3.69.46.111:50000: connect: connection refused\"\nNext Steps\nReferring to the Talos Documentation for how this should be done properly.\nBluntly following Q got something running up in the AWS Clouds. While the instance seemed to be properly running the talosctl service could not connect for some reason. This project was started at a pinch, not prepared for whatsoever so I might have picked an outdated image or try to use a workflow that is not applicable for this AMI . . . .\nThe Q-generated Terraform template is still a far stretch from what is described in the Talos docs. On the other hand that’s what I asked for, a developer instance running Talos so that’s what I got.\nIt would have been nice if it would have worked though. Might just save that for another rainy day.\nWhat is left for now is tf destroy to take things back down again."
  }
]